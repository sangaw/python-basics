{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to parse Azure cloud service data\n",
    "\n",
    "This implementation includes several key features:\n",
    "Web Scraping: Uses requests and BeautifulSoup4 to fetch and parse the Azure VM availability documentation.\n",
    "Content Processing: Removes unnecessary HTML elements, Organizes content into sections based on headers, Maintains the structure of the documentation\n",
    "Question Answering: Uses BERT model fine-tuned on SQuAD (Stanford Question Answering Dataset), Finds the most relevant section for each question, Generates answers based on the context\n",
    "Interactive Interface: Provides a simple command-line chat interface, Allows users to ask questions until they choose to quit\n",
    "To use the chatbot, simply run the script and start asking questions about Azure VM availability. For example:\n",
    "\n",
    "# Example usage\n",
    "chatbot = AzureVMDocChatbot()\n",
    "\n",
    "# Ask questions\n",
    "print(chatbot.answer_question(\"What are availability zones in Azure?\"))\n",
    "print(chatbot.answer_question(\"How does Azure Site Recovery help with business continuity?\"))\n",
    "\n",
    "The chatbot will provide answers based on the documentation content. The answers are generated using the BERT model, which understands context and can provide relevant responses based on the documentation.\n",
    "Some example questions you can ask:\n",
    "What are availability zones?\n",
    "How does Azure ensure storage redundancy?\n",
    "What is an availability set?\n",
    "How does Azure Site Recovery work?\n",
    "What is a Virtual Machine Scale Set?\n",
    "Note that the quality of answers depends on:\n",
    "The relevance of the section matching\n",
    "The quality of the question\n",
    "The presence of the information in the documentation\n",
    "You can enhance this implementation by:\n",
    "Adding better error handling\n",
    "Implementing more sophisticated section matching\n",
    "Adding support for follow-up questions\n",
    "Implementing a web interface instead of command-line\n",
    "Adding support for multiple documentation sources\n",
    "Would you like me to explain any particular part of the implementation in more detail?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing chatbot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:__main__:Models loaded successfully in 8.26 seconds!\n",
      "INFO:__main__:No cached data found. Starting fresh crawl...\n",
      "INFO:__main__:Starting content crawling and parsing...\n",
      "Processing pages:   0%|          | 0/10 [00:00<?, ?it/s]INFO:__main__:\n",
      "Processing: https://learn.microsoft.com/en-us/azure/virtual-machines/\n",
      "Processing pages:  10%|█         | 1/10 [00:00<00:02,  3.32it/s]INFO:__main__:\n",
      "Processing: https://learn.microsoft.com/en-us/azure/virtual-machines/linux/quick-create-terraform\n",
      "Processing pages:  20%|██        | 2/10 [00:00<00:02,  3.85it/s]INFO:__main__:\n",
      "Processing: https://learn.microsoft.com/en-us/azure/virtual-machines/windows/quick-create-portal\n",
      "Processing pages:  30%|███       | 3/10 [00:00<00:01,  4.10it/s]INFO:__main__:\n",
      "Processing: https://learn.microsoft.com/en-us/azure/virtual-machines/hibernate-resume\n",
      "Processing pages:  40%|████      | 4/10 [00:00<00:01,  4.40it/s]INFO:__main__:\n",
      "Processing: https://learn.microsoft.com/en-us/azure/virtual-machines/flexible-virtual-machine-scale-sets\n",
      "Processing pages:  50%|█████     | 5/10 [00:01<00:01,  4.54it/s]INFO:__main__:\n",
      "Processing: https://learn.microsoft.com/en-us/azure/virtual-machines/linux/quick-create-powershell\n",
      "Processing pages:  60%|██████    | 6/10 [00:01<00:01,  3.18it/s]INFO:__main__:\n",
      "Processing: https://learn.microsoft.com/en-us/azure/virtual-machines/windows/quick-create-terraform\n",
      "Processing pages:  70%|███████   | 7/10 [00:01<00:00,  3.30it/s]INFO:__main__:\n",
      "Processing: https://learn.microsoft.com/en-us/azure/virtual-machines/windows/quick-create-cli\n",
      "Processing pages:  80%|████████  | 8/10 [00:02<00:00,  3.68it/s]INFO:__main__:\n",
      "Processing: https://learn.microsoft.com/en-us/azure/virtual-machines/linux/quick-create-cli\n",
      "Processing pages:  90%|█████████ | 9/10 [00:02<00:00,  2.90it/s]INFO:__main__:\n",
      "Processing: https://learn.microsoft.com/en-us/troubleshoot/azure/virtual-machines/welcome-deployment-troubleshooting\n",
      "Processing pages: 100%|██████████| 10/10 [00:03<00:00,  2.88it/s]\n",
      "INFO:__main__:\n",
      "Crawling completed in 3.48 seconds\n",
      "INFO:__main__:Processed 10 pages\n",
      "INFO:__main__:Generating embeddings for sections...\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.31it/s] ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.95it/s]:04, 19.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.98it/s]:03, 26.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 67.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.15it/s]:06, 13.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 78.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.60it/s]0:04, 18.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 58.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.58it/s]0:04, 17.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.48it/s]0:03, 18.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 52.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.45it/s]0:03, 19.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 82.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.02it/s]0:03, 20.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.35it/s]0:03, 19.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 58.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.51it/s]0:02, 21.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.55it/s]0:02, 21.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 52.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.97it/s]0:02, 21.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.03it/s]0:02, 20.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 52.12it/s]0:02, 23.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 47.05it/s]0:02, 22.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.44it/s]0:02, 17.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.78it/s]0:02, 16.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.94it/s]0:02, 16.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.13it/s]0:02, 16.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.56it/s]0:01, 19.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.95it/s]0:01, 17.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.69it/s]0:01, 15.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.17it/s]0:02, 11.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.71it/s]0:01, 13.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.29it/s]0:01, 13.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.04it/s]0:01, 14.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.70it/s]0:01, 12.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.23it/s]0:00, 15.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.30it/s]0:00, 14.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.35it/s]0:00, 12.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.97it/s]0:00, 10.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.30it/s]0:00, 10.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.96it/s]0:00, 12.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 51.76it/s]0:00, 14.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.01it/s]\n",
      "Generating embeddings: 100%|██████████| 91/91 [00:05<00:00, 16.41it/s]\n",
      "INFO:__main__:Generated embeddings for 91 sections\n",
      "INFO:__main__:Saving cached data...\n",
      "INFO:__main__:Cache saved successfully!\n",
      "INFO:__main__:\n",
      "Processing question: What are Azure Virtual Machines?\n",
      "INFO:__main__:Searching for relevant section...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What are Azure Virtual Machines?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.62it/s]\n",
      "INFO:__main__:Found relevant section in 0.08 seconds\n",
      "INFO:__main__:Source: https://learn.microsoft.com/en-us/azure/virtual-machines/\n",
      "INFO:__main__:Generating answer...\n",
      "INFO:__main__:Answer generated in 4.73 seconds\n",
      "INFO:__main__:\n",
      "Processing question: How do I create a VM in Azure?\n",
      "INFO:__main__:Searching for relevant section...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: documentation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q: How do I create a VM in Azure?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.65it/s]\n",
      "INFO:__main__:Found relevant section in 0.04 seconds\n",
      "INFO:__main__:Source: https://learn.microsoft.com/en-us/azure/virtual-machines/\n",
      "INFO:__main__:Generating answer...\n",
      "INFO:__main__:Answer generated in 0.29 seconds\n",
      "INFO:__main__:\n",
      "Processing question: What are the latest features in Azure VMs?\n",
      "INFO:__main__:Searching for relevant section...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: documentation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q: What are the latest features in Azure VMs?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.81it/s]\n",
      "INFO:__main__:Found relevant section in 0.03 seconds\n",
      "INFO:__main__:Source: https://learn.microsoft.com/en-us/azure/virtual-machines/\n",
      "INFO:__main__:Generating answer...\n",
      "INFO:__main__:Answer generated in 0.31 seconds\n",
      "INFO:__main__:\n",
      "Processing question: What is Azure Boost?\n",
      "INFO:__main__:Searching for relevant section...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: documentation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q: What is Azure Boost?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.89it/s]\n",
      "INFO:__main__:Found relevant section in 0.06 seconds\n",
      "INFO:__main__:Source: https://learn.microsoft.com/en-us/azure/virtual-machines/\n",
      "INFO:__main__:Generating answer...\n",
      "INFO:__main__:Answer generated in 0.33 seconds\n",
      "INFO:__main__:\n",
      "Processing question: How does VM hibernation work?\n",
      "INFO:__main__:Searching for relevant section...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: virtual machine scale sets\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q: How does VM hibernation work?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.49it/s]\n",
      "INFO:__main__:Found relevant section in 0.05 seconds\n",
      "INFO:__main__:Source: https://learn.microsoft.com/en-us/azure/virtual-machines/hibernate-resume\n",
      "INFO:__main__:Generating answer...\n",
      "INFO:__main__:Answer generated in 0.53 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: check out thewindows hibernation documentation\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class AzureVMDocChatbot:\n",
    "    def __init__(self, timeout=30, max_depth=2, max_pages=10, data_dir='cached_data'):\n",
    "        # Set up logging first\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Initialize attributes\n",
    "        self.base_url = \"https://learn.microsoft.com/en-us/azure/virtual-machines/\"\n",
    "        self.doc_content = \"\"\n",
    "        self.sections = {}\n",
    "        self.timeout = timeout\n",
    "        self.max_depth = max_depth\n",
    "        self.max_pages = max_pages\n",
    "        self.visited_urls = set()\n",
    "        self.url_to_sections = defaultdict(dict)\n",
    "        self.data_dir = data_dir\n",
    "        self.embeddings = {}\n",
    "        \n",
    "        # Create data directory if it doesn't exist\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize models\n",
    "        self.init_models()\n",
    "        \n",
    "        # Try to load cached data first, if not available then crawl\n",
    "        if not self.load_cached_data():\n",
    "            self.logger.info(\"No cached data found. Starting fresh crawl...\")\n",
    "            self.crawl_and_parse_content()\n",
    "            self.generate_embeddings()\n",
    "            self.save_cached_data()\n",
    "\n",
    "    def init_models(self):\n",
    "        \"\"\"Initialize the BERT and sentence transformer models\"\"\"\n",
    "        self.logger.info(\"Loading models...\")\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # QA model\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                \"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    "                local_files_only=False\n",
    "            )\n",
    "            self.qa_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "                \"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    "                local_files_only=False\n",
    "            )\n",
    "            \n",
    "            # Sentence embedding model\n",
    "            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            \n",
    "            self.logger.info(f\"Models loaded successfully in {time.time() - start_time:.2f} seconds!\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading models: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def save_cached_data(self):\n",
    "        \"\"\"Save parsed content and embeddings to disk\"\"\"\n",
    "        self.logger.info(\"Saving cached data...\")\n",
    "        try:\n",
    "            # Save sections and URL mapping\n",
    "            with open(os.path.join(self.data_dir, 'sections.json'), 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.sections, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            with open(os.path.join(self.data_dir, 'url_to_sections.json'), 'w', encoding='utf-8') as f:\n",
    "                json.dump(dict(self.url_to_sections), f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            # Save embeddings using pickle\n",
    "            with open(os.path.join(self.data_dir, 'embeddings.pkl'), 'wb') as f:\n",
    "                pickle.dump(self.embeddings, f)\n",
    "                \n",
    "            self.logger.info(\"Cache saved successfully!\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving cache: {str(e)}\")\n",
    "\n",
    "    def load_cached_data(self):\n",
    "        \"\"\"Load cached content and embeddings if available\"\"\"\n",
    "        try:\n",
    "            # Check if all required cache files exist\n",
    "            cache_files = ['sections.json', 'url_to_sections.json', 'embeddings.pkl']\n",
    "            if not all(os.path.exists(os.path.join(self.data_dir, f)) for f in cache_files):\n",
    "                return False\n",
    "                \n",
    "            self.logger.info(\"Loading cached data...\")\n",
    "            \n",
    "            # Load sections and URL mapping\n",
    "            with open(os.path.join(self.data_dir, 'sections.json'), 'r', encoding='utf-8') as f:\n",
    "                self.sections = json.load(f)\n",
    "            \n",
    "            with open(os.path.join(self.data_dir, 'url_to_sections.json'), 'r', encoding='utf-8') as f:\n",
    "                self.url_to_sections = defaultdict(dict, json.load(f))\n",
    "            \n",
    "            # Load embeddings\n",
    "            with open(os.path.join(self.data_dir, 'embeddings.pkl'), 'rb') as f:\n",
    "                self.embeddings = pickle.load(f)\n",
    "                \n",
    "            self.logger.info(\"Cache loaded successfully!\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading cache: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def is_valid_azure_doc_url(self, url):\n",
    "        \"\"\"Check if URL is a valid Azure VM documentation page\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        return (\n",
    "            parsed.netloc == \"learn.microsoft.com\" and\n",
    "            \"/azure/virtual-machines\" in parsed.path and\n",
    "            not any(ext in url for ext in ['.png', '.jpg', '.gif', '.pdf'])\n",
    "        )\n",
    "\n",
    "    def extract_links(self, soup, current_url):\n",
    "        \"\"\"Extract valid Azure documentation links from the page\"\"\"\n",
    "        links = set()\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href']\n",
    "            absolute_url = urljoin(current_url, href)\n",
    "            if self.is_valid_azure_doc_url(absolute_url) and absolute_url not in self.visited_urls:\n",
    "                links.add(absolute_url)\n",
    "        return links\n",
    "\n",
    "    def parse_page_content(self, soup, url):\n",
    "        \"\"\"Parse content from a single page\"\"\"\n",
    "        main_content = soup.find('main')\n",
    "        if not main_content:\n",
    "            return\n",
    "        \n",
    "        # Remove unnecessary elements\n",
    "        for element in main_content.find_all(['script', 'style', 'nav']):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Parse sections\n",
    "        headers = main_content.find_all(['h1', 'h2', 'h3'])\n",
    "        current_section = \"\"\n",
    "        current_content = []\n",
    "        \n",
    "        for header in headers:\n",
    "            if current_section:\n",
    "                self.url_to_sections[url][current_section] = ' '.join(current_content)\n",
    "            current_section = header.get_text(strip=True)\n",
    "            current_content = []\n",
    "            \n",
    "            next_element = header.find_next_sibling()\n",
    "            while next_element and not next_element.name in ['h1', 'h2', 'h3']:\n",
    "                if next_element.get_text(strip=True):\n",
    "                    current_content.append(next_element.get_text(strip=True))\n",
    "                next_element = next_element.find_next_sibling()\n",
    "        \n",
    "        # Add the last section\n",
    "        if current_section:\n",
    "            self.url_to_sections[url][current_section] = ' '.join(current_content)\n",
    "\n",
    "    def crawl_and_parse_content(self):\n",
    "        \"\"\"Crawl through pages and parse content with depth limit\"\"\"\n",
    "        try:\n",
    "            to_visit = [(self.base_url, 0)]  # (url, depth)\n",
    "            pages_processed = 0\n",
    "            \n",
    "            self.logger.info(\"Starting content crawling and parsing...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            with tqdm(total=self.max_pages, desc=\"Processing pages\") as pbar:\n",
    "                while to_visit and pages_processed < self.max_pages:\n",
    "                    current_url, depth = to_visit.pop(0)\n",
    "                    \n",
    "                    if current_url in self.visited_urls:\n",
    "                        continue\n",
    "                    \n",
    "                    self.logger.info(f\"\\nProcessing: {current_url}\")\n",
    "                    try:\n",
    "                        response = requests.get(current_url, timeout=self.timeout)\n",
    "                        response.raise_for_status()\n",
    "                        \n",
    "                        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                        self.parse_page_content(soup, current_url)\n",
    "                        self.visited_urls.add(current_url)\n",
    "                        pages_processed += 1\n",
    "                        pbar.update(1)\n",
    "                        \n",
    "                        # If we haven't reached max depth, add child links to visit\n",
    "                        if depth < self.max_depth:\n",
    "                            new_links = self.extract_links(soup, current_url)\n",
    "                            to_visit.extend((link, depth + 1) for link in new_links)\n",
    "                            \n",
    "                    except requests.RequestException as e:\n",
    "                        self.logger.error(f\"Error fetching {current_url}: {str(e)}\")\n",
    "                        continue\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            self.logger.info(f\"\\nCrawling completed in {total_time:.2f} seconds\")\n",
    "            self.logger.info(f\"Processed {pages_processed} pages\")\n",
    "            \n",
    "            # Combine all sections for searching\n",
    "            for url, sections in self.url_to_sections.items():\n",
    "                self.sections.update({f\"{k} ({url})\": v for k, v in sections.items()})\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during crawling: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self):\n",
    "        \"\"\"Generate embeddings for all sections\"\"\"\n",
    "        self.logger.info(\"Generating embeddings for sections...\")\n",
    "        try:\n",
    "            for section_key, content in tqdm(self.sections.items(), desc=\"Generating embeddings\"):\n",
    "                # Generate embedding for the content\n",
    "                embedding = self.embedding_model.encode(content)\n",
    "                self.embeddings[section_key] = embedding\n",
    "            \n",
    "            self.logger.info(f\"Generated embeddings for {len(self.embeddings)} sections\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error generating embeddings: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def find_most_relevant_section(self, question):\n",
    "        \"\"\"Find the most relevant section using embeddings\"\"\"\n",
    "        self.logger.info(\"Searching for relevant section...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Generate embedding for the question\n",
    "            question_embedding = self.embedding_model.encode(question)\n",
    "            \n",
    "            # Find most similar section using cosine similarity\n",
    "            max_similarity = -1\n",
    "            best_section = None\n",
    "            best_url = None\n",
    "            \n",
    "            for section_key, section_embedding in self.embeddings.items():\n",
    "                similarity = np.dot(question_embedding, section_embedding) / (\n",
    "                    np.linalg.norm(question_embedding) * np.linalg.norm(section_embedding)\n",
    "                )\n",
    "                \n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    content = self.sections[section_key]\n",
    "                    best_section = content\n",
    "                    best_url = section_key[section_key.rfind(\"(\")+1:-1] if \"(\" in section_key else None\n",
    "            \n",
    "            search_time = time.time() - start_time\n",
    "            self.logger.info(f\"Found relevant section in {search_time:.2f} seconds\")\n",
    "            if best_url:\n",
    "                self.logger.info(f\"Source: {best_url}\")\n",
    "            return best_section\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error finding relevant section: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def answer_question(self, question):\n",
    "        \"\"\"Answer a question using the crawled content\"\"\"\n",
    "        self.logger.info(f\"\\nProcessing question: {question}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        context = self.find_most_relevant_section(question)\n",
    "        if not context:\n",
    "            return \"I'm sorry, I couldn't find relevant information to answer your question.\"\n",
    "\n",
    "        self.logger.info(\"Generating answer...\")\n",
    "        inputs = self.tokenizer(question, context, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.qa_model(**inputs)\n",
    "        \n",
    "        answer_start = torch.argmax(outputs.start_logits)\n",
    "        answer_end = torch.argmax(outputs.end_logits)\n",
    "        \n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "        answer = tokens[answer_start:answer_end + 1]\n",
    "        answer = self.tokenizer.convert_tokens_to_string(answer)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        self.logger.info(f\"Answer generated in {total_time:.2f} seconds\")\n",
    "        \n",
    "        return answer if answer else \"I'm sorry, I couldn't generate a good answer for that question.\"\n",
    "\n",
    "    def demonstrate_embeddings(self):\n",
    "        \"\"\"Demonstrate how embeddings work with simple examples\"\"\"\n",
    "        self.logger.info(\"Demonstrating embeddings...\")\n",
    "        \n",
    "        # Example texts\n",
    "        texts = [\n",
    "            \"Azure Virtual Machines\",\n",
    "            \"Azure VMs\",\n",
    "            \"Creating a VM in Azure\",\n",
    "            \"Kubernetes cluster\",  # Different concept\n",
    "        ]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.embedding_model.encode(texts)\n",
    "        \n",
    "        # Compare similarities\n",
    "        self.logger.info(\"\\nSimilarity scores:\")\n",
    "        for i, text1 in enumerate(texts):\n",
    "            for j, text2 in enumerate(texts[i+1:], i+1):\n",
    "                similarity = np.dot(embeddings[i], embeddings[j]) / (\n",
    "                    np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j])\n",
    "                )\n",
    "                self.logger.info(f\"'{text1}' vs '{text2}': {similarity:.3f}\")\n",
    "\n",
    "def test_chatbot():\n",
    "    print(\"Initializing chatbot...\")\n",
    "    chatbot = AzureVMDocChatbot(timeout=30, max_depth=2, max_pages=10)\n",
    "    \n",
    "    # Test some questions\n",
    "    test_questions = [\n",
    "        \"What are Azure Virtual Machines?\",\n",
    "        \"How do I create a VM in Azure?\",\n",
    "        \"What are the latest features in Azure VMs?\",\n",
    "        \"What is Azure Boost?\",\n",
    "        \"How does VM hibernation work?\"\n",
    "    ]\n",
    "    \n",
    "    for question in test_questions:\n",
    "        print(f\"\\nQ: {question}\")\n",
    "        answer = chatbot.answer_question(question)\n",
    "        print(f\"A: {answer}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading models...\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:__main__:Models loaded successfully in 3.21 seconds!\n",
      "INFO:__main__:Loading cached data...\n",
      "INFO:__main__:Cache loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Initialize the chatbot\n",
    "chatbot = AzureVMDocChatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sections: 91\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Section</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Virtual machines in Azure (https://learn.micro...</td>\n",
       "      <td>Documentation for creating and managing virtua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Latest features (https://learn.microsoft.com/e...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What's new (https://learn.microsoft.com/en-us/...</td>\n",
       "      <td>Azure BoostHibernationFlexible Virtual Machine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linux quickstarts (https://learn.microsoft.com...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quickstart (https://learn.microsoft.com/en-us/...</td>\n",
       "      <td>Azure portalAzure PowerShellTerraformAzure CLI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Section  \\\n",
       "0  Virtual machines in Azure (https://learn.micro...   \n",
       "1  Latest features (https://learn.microsoft.com/e...   \n",
       "2  What's new (https://learn.microsoft.com/en-us/...   \n",
       "3  Linux quickstarts (https://learn.microsoft.com...   \n",
       "4  Quickstart (https://learn.microsoft.com/en-us/...   \n",
       "\n",
       "                                             Content  \n",
       "0  Documentation for creating and managing virtua...  \n",
       "1                                                     \n",
       "2  Azure BoostHibernationFlexible Virtual Machine...  \n",
       "3                                                     \n",
       "4     Azure portalAzure PowerShellTerraformAzure CLI  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Visualize the parsed sections\n",
    "sections_df = pd.DataFrame(list(chatbot.sections.items()), columns=['Section', 'Content'])\n",
    "print(f\"Total number of sections: {len(sections_df)}\")\n",
    "sections_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Processing question: What are availability zones?\n",
      "INFO:__main__:Searching for relevant section...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What are availability zones?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.38it/s]\n",
      "INFO:__main__:Found relevant section in 0.13 seconds\n",
      "INFO:__main__:Source: https://learn.microsoft.com/en-us/azure/virtual-machines/flexible-virtual-machine-scale-sets\n",
      "INFO:__main__:Generating answer...\n",
      "INFO:__main__:Answer generated in 2.91 seconds\n",
      "INFO:__main__:\n",
      "Processing question: How does Azure ensure storage redundancy?\n",
      "INFO:__main__:Searching for relevant section...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: fault domains\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q: How does Azure ensure storage redundancy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.58it/s]\n",
      "INFO:__main__:Found relevant section in 0.04 seconds\n",
      "INFO:__main__:Source: https://learn.microsoft.com/en-us/azure/virtual-machines/\n",
      "INFO:__main__:Generating answer...\n",
      "INFO:__main__:Answer generated in 0.45 seconds\n",
      "INFO:__main__:\n",
      "Processing question: What is an availability set?\n",
      "INFO:__main__:Searching for relevant section...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: frameworkazure architecture center\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q: What is an availability set?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.26it/s]\n",
      "INFO:__main__:Found relevant section in 0.03 seconds\n",
      "INFO:__main__:Source: https://learn.microsoft.com/en-us/azure/virtual-machines/flexible-virtual-machine-scale-sets\n",
      "INFO:__main__:Generating answer...\n",
      "INFO:__main__:Answer generated in 2.03 seconds\n",
      "INFO:__main__:\n",
      "Processing question: How does Azure Site Recovery work?\n",
      "INFO:__main__:Searching for relevant section...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [CLS]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q: How does Azure Site Recovery work?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.59it/s]\n",
      "INFO:__main__:Found relevant section in 0.04 seconds\n",
      "INFO:__main__:Source: https://learn.microsoft.com/en-us/azure/virtual-machines/windows/quick-create-cli\n",
      "INFO:__main__:Generating answer...\n",
      "INFO:__main__:Answer generated in 0.70 seconds\n",
      "INFO:__main__:\n",
      "Processing question: What is a Virtual Machine Scale Set?\n",
      "INFO:__main__:Searching for relevant section...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: to see your vm in action, you then rdp to the vm and install the iis web server\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q: What is a Virtual Machine Scale Set?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.04it/s]\n",
      "INFO:__main__:Found relevant section in 0.04 seconds\n",
      "INFO:__main__:Source: https://learn.microsoft.com/en-us/azure/virtual-machines/hibernate-resume\n",
      "INFO:__main__:Generating answer...\n",
      "INFO:__main__:Answer generated in 0.44 seconds\n",
      "INFO:__main__:\n",
      "Processing question: What is the difference between availability zones and availability sets?\n",
      "INFO:__main__:Searching for relevant section...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: inflexible orchestration mode\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q: What is the difference between availability zones and availability sets?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.47it/s]\n",
      "INFO:__main__:Found relevant section in 0.04 seconds\n",
      "INFO:__main__:Source: https://learn.microsoft.com/en-us/azure/virtual-machines/flexible-virtual-machine-scale-sets\n",
      "INFO:__main__:Generating answer...\n",
      "INFO:__main__:Answer generated in 1.49 seconds\n",
      "INFO:__main__:\n",
      "Processing question: How many availability zones are there in an Azure region?\n",
      "INFO:__main__:Searching for relevant section...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: uniform scale sets and flexible scale sets\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q: How many availability zones are there in an Azure region?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.79it/s]\n",
      "INFO:__main__:Found relevant section in 0.03 seconds\n",
      "INFO:__main__:Source: https://learn.microsoft.com/en-us/azure/virtual-machines/\n",
      "INFO:__main__:Generating answer...\n",
      "INFO:__main__:Answer generated in 0.30 seconds\n",
      "INFO:__main__:\n",
      "Processing question: How does Azure Site Recovery help with business continuity?\n",
      "INFO:__main__:Searching for relevant section...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: I'm sorry, I couldn't generate a good answer for that question.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q: How does Azure Site Recovery help with business continuity?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.00it/s]\n",
      "INFO:__main__:Found relevant section in 0.04 seconds\n",
      "INFO:__main__:Source: https://learn.microsoft.com/en-us/azure/virtual-machines/flexible-virtual-machine-scale-sets\n",
      "INFO:__main__:Generating answer...\n",
      "INFO:__main__:Answer generated in 1.75 seconds\n",
      "INFO:__main__:\n",
      "Processing question: How does Azure Load Balancer work with availability zones?\n",
      "INFO:__main__:Searching for relevant section...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [CLS]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q: How does Azure Load Balancer work with availability zones?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.92it/s]\n",
      "INFO:__main__:Found relevant section in 0.04 seconds\n",
      "INFO:__main__:Source: https://learn.microsoft.com/en-us/azure/virtual-machines/flexible-virtual-machine-scale-sets\n",
      "INFO:__main__:Generating answer...\n",
      "INFO:__main__:Answer generated in 1.75 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: distributing vms\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Test some example questions\n",
    "test_questions = [\n",
    "    \"What are availability zones?\",\n",
    "    \"How does Azure ensure storage redundancy?\",\n",
    "    \"What is an availability set?\",\n",
    "    \"How does Azure Site Recovery work?\",\n",
    "    \"What is a Virtual Machine Scale Set?\",\n",
    "    \"What is the difference between availability zones and availability sets?\",\n",
    "    \"How many availability zones are there in an Azure region?\",\n",
    "    \"How does Azure Site Recovery help with business continuity?\",  \n",
    "    \"How does Azure Load Balancer work with availability zones?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {chatbot.answer_question(question)}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Processing question: What is Azure availability zone?\n",
      "INFO:__main__:Searching for relevant section...\n",
      "INFO:__main__:Found relevant section in 0.01 seconds\n",
      "INFO:__main__:Source: https://learn.microsoft.com/en-us/azure/virtual-machines/availability\n",
      "INFO:__main__:Generating answer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is Azure availability zone?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Answer generated in 1.09 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: a physically separate zone\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Interactive question answering\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def interactive_qa():\n",
    "    while True:\n",
    "        question = input(\"Ask a question (or type 'quit' to exit): \")\n",
    "        if question.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"A: {chatbot.answer_question(question)}\")\n",
    "        print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "interactive_qa()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursor-conda-python-3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
